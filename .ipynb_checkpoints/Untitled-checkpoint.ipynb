{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Darman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Darman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocessor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f8bfb3e37f4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'preprocessor'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from math import log, sqrt\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "\n",
    "# In[124]:\n",
    "\n",
    "\n",
    "tweets = pd.read_csv('socialmedia-disaster-tweets-DFE.csv', encoding = 'latin-1')\n",
    "tweets.head()\n",
    "\n",
    "\n",
    "# In[125]:\n",
    "\n",
    "\n",
    "tweets.rename(columns = {'choose_one': 'labels', 'text': 'message'}, inplace = True)\n",
    "tweets.head()\n",
    "\n",
    "\n",
    "# In[126]:\n",
    "\n",
    "\n",
    "tweets.drop(['_golden', '_unit_id', '_unit_state','_trusted_judgments','_last_judgment_at','choose_one:confidence','choose_one_gold','keyword','location','tweetid','userid'], axis = 1, inplace = True)\n",
    "tweets.head()\n",
    "\n",
    "\n",
    "# In[127]:\n",
    "\n",
    "\n",
    "tweets['label'] = tweets['labels'].map({'Relevant': 1, 'Not Relevant': 0})\n",
    "tweets.drop(['labels'], axis = 1, inplace = True)\n",
    "tweets\n",
    "\n",
    "\n",
    "# In[128]:\n",
    "\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "for index, row in tweets.iterrows():\n",
    "    tweets.set_value(index, 'message', row['message'])\n",
    "    tweets.set_value(index, 'message', BeautifulSoup(row['message'], 'lxml'))\n",
    "    tweets.set_value(index, 'message', re.sub(r'@[A-Za-z0-9]+','',row['message']))\n",
    "    tweets.set_value(index, 'message', re.sub('https?://[A-Za-z0-9./]+','',row['message']))\n",
    "    tweets.set_value(index, 'message', re.sub(\"[^a-zA-Z]\", \" \", row['message']))\n",
    "\n",
    "tweets\n",
    "\n",
    "\n",
    "# In[129]:\n",
    "\n",
    "\n",
    "\n",
    "trainIndex, testIndex = list(), list()\n",
    "for i in range(tweets.shape[0]):\n",
    "    if np.random.uniform(0, 1) < 0.75:\n",
    "        trainIndex += [i]\n",
    "    else:\n",
    "        testIndex += [i]\n",
    "trainData = tweets.loc[trainIndex]\n",
    "testData = tweets.loc[testIndex]\n",
    "trainData.size\n",
    "\n",
    "\n",
    "# In[130]:\n",
    "\n",
    "\n",
    "trainData.reset_index(inplace = True)\n",
    "trainData.drop(['index'], axis = 1, inplace = True)\n",
    "trainData.head()\n",
    "\n",
    "\n",
    "# In[131]:\n",
    "\n",
    "\n",
    "testData.reset_index(inplace = True)\n",
    "testData.drop(['index'], axis = 1, inplace = True)\n",
    "testData.head()\n",
    "\n",
    "\n",
    "# In[132]:\n",
    "\n",
    "\n",
    "disaster_words = ' '.join(list(tweets[tweets['label'] == 1]['message']))\n",
    "disaster_wc = WordCloud(width = 512,height = 512).generate(disaster_words)\n",
    "plt.figure(figsize = (10, 8), facecolor = 'k')\n",
    "plt.imshow(disaster_wc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[133]:\n",
    "\n",
    "\n",
    "normal_words = ' '.join(list(tweets[tweets['label'] == 0]['message']))\n",
    "normal_wc = WordCloud(width = 512,height = 512).generate(normal_words)\n",
    "plt.figure(figsize = (10, 8), facecolor = 'k')\n",
    "plt.imshow(normal_wc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[134]:\n",
    "\n",
    "\n",
    "def process_message(message, lower_case = True, stem = True, stop_words = True, gram = 2):\n",
    "    if lower_case:\n",
    "        message = message.lower()\n",
    "    words = word_tokenize(message)\n",
    "    words = [w for w in words if len(w) > 2]\n",
    "    if gram > 1:\n",
    "        w = []\n",
    "        for i in range(len(words) - gram + 1):\n",
    "            w += [' '.join(words[i:i + gram])]\n",
    "        return w\n",
    "    if stop_words:\n",
    "        sw = stopwords.words('english')\n",
    "        words = [word for word in words if word not in sw]\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]   \n",
    "    return words\n",
    "\n",
    "\n",
    "# In[135]:\n",
    "\n",
    "\n",
    "class DisasterClassifier(object):\n",
    "    def __init__(self, trainData, method = 'tf-idf'):\n",
    "        self.tweets, self.labels = trainData['message'], trainData['label']\n",
    "        self.method = method\n",
    "\n",
    "    def train(self):\n",
    "        self.calc_TF_and_IDF()\n",
    "        if self.method == 'tf-idf':\n",
    "            self.calc_TF_IDF()\n",
    "        else:\n",
    "            self.calc_prob()\n",
    "\n",
    "    def calc_prob(self):\n",
    "        self.prob_disaster = dict()\n",
    "        self.prob_normal = dict()\n",
    "        for word in self.tf_disaster:\n",
    "            self.prob_disaster[word] = (self.tf_disaster[word] + 1) / (self.disaster_words +                                                                 len(list(self.tf_disaster.keys())))\n",
    "        for word in self.tf_normal:\n",
    "            self.prob_normal[word] = (self.tf_normal[word] + 1) / (self.normal_words +                                                                 len(list(self.tf_normal.keys())))\n",
    "        self.prob_disaster_mail, self.prob_normal_mail = self.disaster_tweets / self.total_tweets, self.normal_tweets / self.total_tweets \n",
    "\n",
    "\n",
    "    def calc_TF_and_IDF(self):\n",
    "        noOfMessages = self.tweets.shape[0]\n",
    "        self.disaster_tweets, self.normal_tweets = self.labels.value_counts()[1], self.labels.value_counts()[0]\n",
    "        self.total_tweets = self.disaster_tweets + self.normal_tweets\n",
    "        self.disaster_words = 0\n",
    "        self.normal_words = 0\n",
    "        self.tf_disaster = dict()\n",
    "        self.tf_normal = dict()\n",
    "        self.idf_disaster = dict()\n",
    "        self.idf_normal = dict()\n",
    "        for i in range(noOfMessages):\n",
    "            message_processed = process_message(self.tweets[i])\n",
    "            count = list() #To keep track of whether the word has ocured in the message or not.\n",
    "                           #For IDF\n",
    "            for word in message_processed:\n",
    "                if self.labels[i]:\n",
    "                    self.tf_disaster[word] = self.tf_disaster.get(word, 0) + 1\n",
    "                    self.disaster_words += 1\n",
    "                else:\n",
    "                    self.tf_normal[word] = self.tf_normal.get(word, 0) + 1\n",
    "                    self.normal_words += 1\n",
    "                if word not in count:\n",
    "                    count += [word]\n",
    "            for word in count:\n",
    "                if self.labels[i]:\n",
    "                    self.idf_disaster[word] = self.idf_disaster.get(word, 0) + 1\n",
    "                else:\n",
    "                    self.idf_normal[word] = self.idf_normal.get(word, 0) + 1\n",
    "\n",
    "    def calc_TF_IDF(self):\n",
    "        self.prob_disaster = dict()\n",
    "        self.prob_normal = dict()\n",
    "        self.sum_tf_idf_disaster = 0\n",
    "        self.sum_tf_idf_normal = 0\n",
    "        for word in self.tf_disaster:\n",
    "            self.prob_disaster[word] = (self.tf_disaster[word]) * log((self.disaster_tweets + self.normal_tweets)                                                           / (self.idf_disaster[word] + self.idf_normal.get(word, 0)))\n",
    "            self.sum_tf_idf_disaster += self.prob_disaster[word]\n",
    "        for word in self.tf_disaster:\n",
    "            self.prob_disaster[word] = (self.prob_disaster[word] + 1) / (self.sum_tf_idf_disaster + len(list(self.prob_disaster.keys())))\n",
    "            \n",
    "        for word in self.tf_normal:\n",
    "            self.prob_normal[word] = (self.tf_normal[word]) * log((self.disaster_tweets + self.normal_tweets)                                                           / (self.idf_disaster.get(word, 0) + self.idf_normal[word]))\n",
    "            self.sum_tf_idf_normal += self.prob_normal[word]\n",
    "        for word in self.tf_normal:\n",
    "            self.prob_normal[word] = (self.prob_normal[word] + 1) / (self.sum_tf_idf_normal + len(list(self.prob_normal.keys())))\n",
    "            \n",
    "    \n",
    "        self.prob_disaster_mail, self.prob_normal_mail = self.disaster_tweets / self.total_tweets, self.normal_tweets / self.total_tweets \n",
    "                    \n",
    "    def classify(self, processed_message):\n",
    "        pdisaster, pnormal = 0, 0\n",
    "        for word in processed_message:                \n",
    "            if word in self.prob_disaster:\n",
    "                pdisaster += log(self.prob_disaster[word])\n",
    "            else:\n",
    "                if self.method == 'tf-idf':\n",
    "                    pdisaster -= log(self.sum_tf_idf_disaster + len(list(self.prob_disaster.keys())))\n",
    "                else:\n",
    "                    pdisaster -= log(self.disaster_words + len(list(self.prob_disaster.keys())))\n",
    "            if word in self.prob_normal:\n",
    "                pnormal += log(self.prob_normal[word])\n",
    "            else:\n",
    "                if self.method == 'tf-idf':\n",
    "                    pnormal -= log(self.sum_tf_idf_normal + len(list(self.prob_normal.keys()))) \n",
    "                else:\n",
    "                    pnormal -= log(self.normal_words + len(list(self.prob_normal.keys())))\n",
    "            pdisaster += log(self.prob_disaster_mail)\n",
    "            pnormal += log(self.prob_normal_mail)\n",
    "        return pdisaster >= pnormal\n",
    "    \n",
    "    def predict(self, testData):\n",
    "        result = dict()\n",
    "        for (i, message) in enumerate(testData):\n",
    "            processed_message = process_message(message)\n",
    "            result[i] = int(self.classify(processed_message))\n",
    "        return result\n",
    "\n",
    "\n",
    "# In[136]:\n",
    "\n",
    "\n",
    "def metrics(labels, predictions):\n",
    "    true_pos, true_neg, false_pos, false_neg = 0, 0, 0, 0\n",
    "    for i in range(len(labels)):\n",
    "        true_pos += int(labels[i] == 1 and predictions[i] == 1)\n",
    "        true_neg += int(labels[i] == 0 and predictions[i] == 0)\n",
    "        false_pos += int(labels[i] == 0 and predictions[i] == 1)\n",
    "        false_neg += int(labels[i] == 1 and predictions[i] == 0)\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    Fscore = 2 * precision * recall / (precision + recall)\n",
    "    accuracy = (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)\n",
    "\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F-score: \", Fscore)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "\n",
    "# In[137]:\n",
    "\n",
    "\n",
    "dc_tf_idf = DisasterClassifier(trainData, 'tf-idf')\n",
    "dc_tf_idf.train()\n",
    "preds_tf_idf = dc_tf_idf.predict(testData['message'])\n",
    "metrics(testData['label'], preds_tf_idf)\n",
    "\n",
    "\n",
    "# In[138]:\n",
    "\n",
    "\n",
    "dc_bow = DisasterClassifier(trainData, 'bow')\n",
    "dc_bow.train()\n",
    "preds_bow = dc_bow.predict(testData['message'])\n",
    "metrics(testData['label'], preds_bow)\n",
    "\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
